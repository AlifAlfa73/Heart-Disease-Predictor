{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JONBUSUG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import All Library Needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import random\n",
    "import pandas as pd \n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import graphviz\n",
    "import itertools\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn import metrics\n",
    "from sklearn import tree\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error, classification_report, accuracy_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Data From CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "heart_train = pd.read_csv('tubes2_HeartDisease_train.csv')\n",
    "heart_test = pd.read_csv('tubes2_HeartDisease_test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rename Column Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "test_columns = {\n",
    "    'Column1': 'age',\n",
    "    'Column2': 'sex',\n",
    "    'Column3': 'chest_pain_type',\n",
    "    'Column4': 'resting_blood_pressure',\n",
    "    'Column5': 'serum_cholesterol',\n",
    "    'Column6': 'fasting_blood_sugar',\n",
    "    'Column7': 'resting_ecg',\n",
    "    'Column8': 'max_heart_rate_achieved',\n",
    "    'Column9': 'exercise_induced_angina',\n",
    "    'Column10': 'st_depression',\n",
    "    'Column11': 'peak_exercise_st_segment',\n",
    "    'Column12': 'num_major_flourosopy',\n",
    "    'Column13': 'thal'\n",
    "}\n",
    "train_columns = test_columns.copy()\n",
    "train_columns['Column14'] = 'heart_disease_diagnosis'\n",
    "# Rename columns\n",
    "heart_train = heart_train.rename(columns=train_columns)\n",
    "heart_test = heart_test.rename(columns=test_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column data heart train\n",
      "age                          int64\n",
      "sex                          int64\n",
      "chest_pain_type              int64\n",
      "resting_blood_pressure      object\n",
      "serum_cholesterol           object\n",
      "fasting_blood_sugar         object\n",
      "resting_ecg                 object\n",
      "max_heart_rate_achieved     object\n",
      "exercise_induced_angina     object\n",
      "st_depression               object\n",
      "peak_exercise_st_segment    object\n",
      "num_major_flourosopy        object\n",
      "thal                        object\n",
      "heart_disease_diagnosis      int64\n",
      "dtype: object\n",
      "\n",
      "Show heart train head\n",
      "   age  sex  chest_pain_type resting_blood_pressure serum_cholesterol  \\\n",
      "0   54    1                4                    125               216   \n",
      "1   55    1                4                    158               217   \n",
      "2   54    0                3                    135               304   \n",
      "3   48    0                3                    120               195   \n",
      "4   50    1                4                    120                 0   \n",
      "\n",
      "  fasting_blood_sugar resting_ecg max_heart_rate_achieved  \\\n",
      "0                   0           0                     140   \n",
      "1                   0           0                     110   \n",
      "2                   1           0                     170   \n",
      "3                   0           0                     125   \n",
      "4                   0           1                     156   \n",
      "\n",
      "  exercise_induced_angina st_depression peak_exercise_st_segment  \\\n",
      "0                       0             0                        ?   \n",
      "1                       1           2.5                        2   \n",
      "2                       0             0                        1   \n",
      "3                       0             0                        ?   \n",
      "4                       1             0                        1   \n",
      "\n",
      "  num_major_flourosopy thal  heart_disease_diagnosis  \n",
      "0                    ?    ?                        1  \n",
      "1                    ?    ?                        1  \n",
      "2                    0    3                        0  \n",
      "3                    ?    ?                        0  \n",
      "4                    ?    6                        3  \n",
      "\n",
      "Find sum value undefined in each column\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "age                         0\n",
       "sex                         0\n",
       "chest_pain_type             0\n",
       "resting_blood_pressure      0\n",
       "serum_cholesterol           0\n",
       "fasting_blood_sugar         0\n",
       "resting_ecg                 1\n",
       "max_heart_rate_achieved     0\n",
       "exercise_induced_angina     0\n",
       "st_depression               0\n",
       "peak_exercise_st_segment    0\n",
       "num_major_flourosopy        0\n",
       "thal                        0\n",
       "heart_disease_diagnosis     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Column data heart train')\n",
    "pprint(heart_train.dtypes)\n",
    "print()\n",
    "print('Show heart train head')\n",
    "pprint(heart_train.head())\n",
    "print()\n",
    "print('Find sum value undefined in each column')\n",
    "heart_train.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Berdasarkan dengan beberapa pengecekan di atas, dapat dilihat bahwa data pada csv yang diberikan:\n",
    "1. Tidak semua data bertipe numerik\n",
    "2. Ada beberapa data yang bernilai '?'\n",
    "3. Ada data yang bernilai NaN (undefined)\n",
    "\n",
    "Hal tersebut dapat mengganggu proses pemodelan. Oleh karena itu perlu dilakukan pre-processing sebagai berikut :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataframe Conversion to Numeric "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data type of columns after conversion\n",
      "age                          int64\n",
      "sex                          int64\n",
      "chest_pain_type              int64\n",
      "resting_blood_pressure      object\n",
      "serum_cholesterol           object\n",
      "fasting_blood_sugar         object\n",
      "resting_ecg                 object\n",
      "max_heart_rate_achieved     object\n",
      "exercise_induced_angina     object\n",
      "st_depression               object\n",
      "peak_exercise_st_segment    object\n",
      "num_major_flourosopy        object\n",
      "thal                        object\n",
      "heart_disease_diagnosis      int64\n",
      "dtype: object\n",
      "\n",
      "Total value NaN after heart_train converted to numeric value\n",
      "age                           0\n",
      "sex                           0\n",
      "chest_pain_type               0\n",
      "resting_blood_pressure        4\n",
      "serum_cholesterol            21\n",
      "fasting_blood_sugar          78\n",
      "resting_ecg                   2\n",
      "max_heart_rate_achieved       1\n",
      "exercise_induced_angina       1\n",
      "st_depression                 6\n",
      "peak_exercise_st_segment    219\n",
      "num_major_flourosopy        471\n",
      "thal                        367\n",
      "heart_disease_diagnosis       0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Convert string to numeric, convert non-number to NAN\n",
    "# heart_train = heart_train.apply(pd.to_numeric, errors = 'coerce')\n",
    "heart_train = heart_train.replace('?', np.NaN)\n",
    "heart_train = heart_train.dropna(thresh=9)\n",
    "\n",
    "print('Data type of columns after conversion')\n",
    "print(heart_train.dtypes)\n",
    "print()\n",
    "\n",
    "# NaN count\n",
    "print('Total value NaN after heart_train converted to numeric value')\n",
    "print(heart_train.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Menghilangkan nilai NaN\n",
    "\n",
    "Pada pre-processingnya, konversi data dari object (string) menjadi numerik berhasil menghilagkan tipe objek dari dataframe. Namun, untuk value yang tidak dapat dikonversi menjadi angka akan bernilai NaN yang membuat dataframe tidak bisa diolah. \n",
    "\n",
    "Salah satu cara termudah untuk menghilangkan nilai NaN adalah dengan cara menghapus row yang mengandung nilai tersebut, namun melihat pada column 12 terdapat 514 row yang bernilai NaN, cara ini tidak feasible karena akan sangat mengurangi data training. Oleh karena itum, kami memutuskan untuk me-replace nilai NaN dengan XXXXXX. Pemilihan XXXXXX dilakukan karena XXXXXXXXXXXXXXXXXXXXXXXXXXXX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total NaN Value\n",
      "age                         0\n",
      "sex                         0\n",
      "chest_pain_type             0\n",
      "resting_blood_pressure      0\n",
      "serum_cholesterol           0\n",
      "fasting_blood_sugar         0\n",
      "resting_ecg                 0\n",
      "max_heart_rate_achieved     0\n",
      "exercise_induced_angina     0\n",
      "st_depression               0\n",
      "peak_exercise_st_segment    0\n",
      "thal                        0\n",
      "heart_disease_diagnosis     0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Drop num_major_flouroscopy, too many NaNs\n",
    "heart_train = heart_train.drop('num_major_flourosopy', axis=1)\n",
    "heart_train['thal'].fillna(heart_train['thal'].mode()[0], inplace=True)\n",
    "heart_train['peak_exercise_st_segment'].fillna(heart_train['peak_exercise_st_segment'].mode()[0], inplace=True)\n",
    "heart_train['chest_pain_type'].fillna(heart_train['chest_pain_type'].mode()[0], inplace=True)\n",
    "heart_train['resting_ecg'].fillna(heart_train['resting_ecg'].mode()[0], inplace=True)\n",
    "heart_train['fasting_blood_sugar'].fillna(heart_train['fasting_blood_sugar'].mode()[0], inplace=True)\n",
    "\n",
    "imp = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "\n",
    "c = heart_train.columns\n",
    "heart_train = pd.DataFrame(imp.fit_transform(heart_train))\n",
    "heart_train.columns = c\n",
    "\n",
    "# median = heart_train.median(axis=0)\n",
    "# for idx, column in enumerate(heart_train.columns):\n",
    "#     heart_train[column] = heart_train[column].replace(np.NaN, median[idx])\n",
    "\n",
    "# Count NaN value\n",
    "print('Total NaN Value')\n",
    "print(heart_train.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_chest_pain(row):\n",
    "    if row['chest_pain_type'] == 1: return 1\n",
    "    else: return 0\n",
    "\n",
    "def label_chest_pain_2(row):\n",
    "    if row['chest_pain_type'] == 2: return 1\n",
    "    else: return 0\n",
    "    \n",
    "def label_chest_pain_3(row):\n",
    "    if row['chest_pain_type'] == 3: return 1\n",
    "    else: return 0\n",
    "    \n",
    "def label_chest_pain_4(row):\n",
    "    if row['chest_pain_type'] == 4: return 1\n",
    "    else: return 0\n",
    "\n",
    "heart_train['typical_angina'] = heart_train.apply(lambda row: label_chest_pain(row), axis=1)\n",
    "heart_train['atypical_angina'] = heart_train.apply(lambda row: label_chest_pain_2(row), axis=1)\n",
    "heart_train['non_anginal_pain'] = heart_train.apply(lambda row: label_chest_pain_3(row), axis=1)\n",
    "heart_train['asymptotic'] = heart_train.apply(lambda row: label_chest_pain_4(row), axis=1)\n",
    "heart_train = heart_train.drop('chest_pain_type', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upsloping(row):\n",
    "    if row[\"peak_exercise_st_segment\"] == 1: return 1\n",
    "    else: return 0\n",
    "\n",
    "def flat(row):\n",
    "    if row[\"peak_exercise_st_segment\"] == 2: return 1\n",
    "    else: return 0\n",
    "    \n",
    "def downsloping(row):\n",
    "    if row[\"peak_exercise_st_segment\"] == 3: return 1\n",
    "    else: return 0\n",
    "    \n",
    "heart_train['upsloping'] = heart_train.apply(lambda row: upsloping(row), axis=1)\n",
    "heart_train['flat'] = heart_train.apply(lambda row: flat(row), axis=1)\n",
    "heart_train['downsloping'] = heart_train.apply(lambda row: downsloping(row), axis=1)\n",
    "heart_train = heart_train.drop('peak_exercise_st_segment', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binarize resting_ecg, normal as 0 and everything else as 1\n",
    "# heart_train['resting_ecg'] = (heart_train['resting_ecg'] >= 1).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binarize resting blood pressure to 0 for [120, 140] (healthy) and 1 otherwise\n",
    "# heart_train['resting_blood_pressure'] = ((heart_train['resting_blood_pressure'] > 140) | (heart_train['resting_blood_pressure'] < 120)).astype(int)\n",
    "# heart_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joseph/Assignments/Heart-Disease-Predictor/venv/lib/python3.6/site-packages/sklearn/preprocessing/data.py:323: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by MinMaxScaler.\n",
      "  return self.partial_fit(X, y)\n"
     ]
    }
   ],
   "source": [
    "# Split data train\n",
    "# heart_train_copy = heart_train.copy()\n",
    "Y = heart_train['heart_disease_diagnosis']\n",
    "X = heart_train.drop('heart_disease_diagnosis', axis = 1)\n",
    "\n",
    "# scaler = StandardScaler().fit(X)\n",
    "# X = scaler.transform(X)\n",
    "\n",
    "# Best so far\n",
    "scaler = MinMaxScaler(feature_range=(0,1))\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "KF = KFold(10, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algoritma Naive-Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'fit_time': array([0.00144219, 0.00095463, 0.00114965, 0.00095272, 0.00094557,\n",
      "       0.00097489, 0.00098896, 0.00125766, 0.00114202, 0.00099063]),\n",
      " 'score_time': array([0.00049019, 0.00040913, 0.00040722, 0.00039148, 0.00037789,\n",
      "       0.00052905, 0.00039315, 0.00046015, 0.00045228, 0.00037694]),\n",
      " 'test_score': array([0.52631579, 0.53333333, 0.56      , 0.56756757, 0.58108108,\n",
      "       0.50684932, 0.5890411 , 0.5890411 , 0.58333333, 0.57746479]),\n",
      " 'train_score': array([0.6030303 , 0.602118  , 0.60514372, 0.60876133, 0.59516616,\n",
      "       0.60935143, 0.59879336, 0.59276018, 0.59789157, 0.59398496])}\n",
      "0.5614027400370709\n"
     ]
    }
   ],
   "source": [
    "gnb = GaussianNB()\n",
    "\n",
    "results = cross_validate(gnb, X, Y, cv=10)\n",
    "pprint(results)\n",
    "\n",
    "print(sum(results['test_score'])/10)\n",
    "\n",
    "# i=1\n",
    "# sum_acc = 0\n",
    "# sum_prec = 0\n",
    "# sum_rec = 0\n",
    "# for trainidx, testidx in KF.split(X):\n",
    "#     X_train, X_test = X.iloc[trainidx], X.iloc[testidx]\n",
    "#     Y_train, Y_test = Y.iloc[trainidx], Y.iloc[testidx]\n",
    "#     gnb.fit(X_train,Y_train)\n",
    "\n",
    "#     accuration = metrics.accuracy_score(Y_test, gnb.predict(X_test))\n",
    "#     precision = metrics.precision_score(Y_test, gnb.predict(X_test), average=\"macro\")\n",
    "#     recall = metrics.recall_score(Y_test, gnb.predict(X_test), average=\"macro\")\n",
    "    \n",
    "#     i+=1\n",
    "#     sum_acc += accuration\n",
    "#     sum_prec += precision\n",
    "#     sum_rec += recall\n",
    "    \n",
    "# print(\"Average Accuration : {0:.4f}\".format(sum_acc/10))\n",
    "# print(\"Average Precision : {0:.4f}\".format(sum_prec/10))\n",
    "# print(\"Average Recall : {0:.4f}\".format(sum_rec/10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algoritma KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'fit_time': array([0.00093961, 0.00108695, 0.00079322, 0.0007894 , 0.00073552,\n",
      "       0.00069547, 0.00073433, 0.00070667, 0.00068331, 0.00070596]),\n",
      " 'score_time': array([0.00197411, 0.00214195, 0.00205088, 0.00195265, 0.00188208,\n",
      "       0.00183415, 0.00181556, 0.00178123, 0.00175333, 0.00189757]),\n",
      " 'test_score': array([0.55263158, 0.61333333, 0.57333333, 0.62162162, 0.58108108,\n",
      "       0.54794521, 0.57534247, 0.54794521, 0.55555556, 0.63380282]),\n",
      " 'train_score': array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])}\n",
      "0.580259219748603\n"
     ]
    }
   ],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=21, weights='distance')\n",
    "\n",
    "results = cross_validate(knn, X, Y, cv=10)\n",
    "pprint(results)\n",
    "\n",
    "print(sum(results['test_score'])/10)\n",
    "# i=1\n",
    "# sum_acc = 0\n",
    "# sum_prec = 0\n",
    "# sum_rec = 0\n",
    "# for trainidx, testidx in KF.split(X):\n",
    "#     X_train, X_test = X.iloc[trainidx], X.iloc[testidx]\n",
    "#     Y_train, Y_test = Y.iloc[trainidx], Y.iloc[testidx]\n",
    "#     knn.fit(X_train,Y_train)\n",
    "\n",
    "#     accuration = metrics.accuracy_score(Y_test, knn.predict(X_test))\n",
    "#     precision = metrics.precision_score(Y_test, knn.predict(X_test), average=\"macro\")\n",
    "#     recall = metrics.recall_score(Y_test, knn.predict(X_test), average=\"macro\")\n",
    "\n",
    "#     i+=1\n",
    "#     sum_acc += accuration\n",
    "#     sum_prec += precision\n",
    "#     sum_rec += recall\n",
    "    \n",
    "# print(\"Average Accuration : {0:.4f}\".format(sum_acc/10))\n",
    "# print(\"Average Precision : {0:.4f}\".format(sum_prec/10))\n",
    "# print(\"Average Recall : {0:.4f}\".format(sum_rec/10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algoritma MLP "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'fit_time': array([0.21113229, 0.20123005, 0.21560025, 0.19209337, 0.17941642,\n",
      "       0.22547412, 0.20017695, 0.1860261 , 0.18797183, 0.17953992]),\n",
      " 'score_time': array([0.00039673, 0.00038338, 0.00033236, 0.00035906, 0.00032187,\n",
      "       0.00038862, 0.00033355, 0.00031924, 0.00033188, 0.00031471]),\n",
      " 'test_score': array([0.56578947, 0.65333333, 0.54666667, 0.59459459, 0.59459459,\n",
      "       0.57534247, 0.61643836, 0.53424658, 0.54166667, 0.66197183]),\n",
      " 'train_score': array([0.59393939, 0.59152799, 0.60816944, 0.60120846, 0.59365559,\n",
      "       0.59426848, 0.60331825, 0.59125189, 0.59789157, 0.59548872])}\n",
      "0.5884644557786256\n"
     ]
    }
   ],
   "source": [
    "mlp = MLPClassifier(hidden_layer_sizes=(2), solver='sgd', \n",
    "                    max_iter=500, learning_rate_init=0.1, learning_rate='adaptive',\n",
    "                    activation='identity')\n",
    "\n",
    "results = cross_validate(mlp, X, Y, cv=10)\n",
    "pprint(results)\n",
    "\n",
    "print(sum(results['test_score'])/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mlp.fit(X[100:], Y[100:])\n",
    "# mlp.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
