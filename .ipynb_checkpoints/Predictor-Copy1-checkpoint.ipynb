{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignments IF3170 - Artificial Intelligent\n",
    "## Web Application for classifying hearth disease from data clinic \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase A - Find Best Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Group 3 - Unexpected\n",
    "    - Rizki Alif Salman Alfarisy / 13516005\n",
    "    - Jonathan Alvaro / 13516023\n",
    "    - Joseph Salimin / 13516037\n",
    "    - Kevin Leonardo Limitius / 13516049\n",
    "    - Kevin Basuki / 13516071"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First of All.. Import All Library Needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import random\n",
    "import pandas as pd \n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import graphviz\n",
    "import itertools\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn import metrics\n",
    "from sklearn import tree\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error, classification_report, accuracy_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Data From CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "heart_train = pd.read_csv('tubes2_HeartDisease_train.csv')\n",
    "heart_test = pd.read_csv('tubes2_HeartDisease_test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rename Column Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "test_columns = {\n",
    "    'Column1': 'age',\n",
    "    'Column2': 'sex',\n",
    "    'Column3': 'chest_pain_type',\n",
    "    'Column4': 'resting_blood_pressure',\n",
    "    'Column5': 'serum_cholesterol',\n",
    "    'Column6': 'fasting_blood_sugar',\n",
    "    'Column7': 'resting_ecg',\n",
    "    'Column8': 'max_heart_rate_achieved',\n",
    "    'Column9': 'exercise_induced_angina',\n",
    "    'Column10': 'st_depression',\n",
    "    'Column11': 'peak_exercise_st_segment',\n",
    "    'Column12': 'num_major_flourosopy',\n",
    "    'Column13': 'thal'\n",
    "}\n",
    "# Create train columns\n",
    "train_columns = test_columns.copy()\n",
    "train_columns['Column14'] = 'heart_disease_diagnosis'\n",
    "\n",
    "# Rename columns\n",
    "heart_train = heart_train.rename(columns=train_columns)\n",
    "heart_test = heart_test.rename(columns=test_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column data heart train\n",
      "age                          int64\n",
      "sex                          int64\n",
      "chest_pain_type              int64\n",
      "resting_blood_pressure      object\n",
      "serum_cholesterol           object\n",
      "fasting_blood_sugar         object\n",
      "resting_ecg                 object\n",
      "max_heart_rate_achieved     object\n",
      "exercise_induced_angina     object\n",
      "st_depression               object\n",
      "peak_exercise_st_segment    object\n",
      "num_major_flourosopy        object\n",
      "thal                        object\n",
      "heart_disease_diagnosis      int64\n",
      "dtype: object\n",
      "\n",
      "Show heart train head\n",
      "   age  sex  chest_pain_type resting_blood_pressure serum_cholesterol  \\\n",
      "0   54    1                4                    125               216   \n",
      "1   55    1                4                    158               217   \n",
      "2   54    0                3                    135               304   \n",
      "3   48    0                3                    120               195   \n",
      "4   50    1                4                    120                 0   \n",
      "\n",
      "  fasting_blood_sugar resting_ecg max_heart_rate_achieved  \\\n",
      "0                   0           0                     140   \n",
      "1                   0           0                     110   \n",
      "2                   1           0                     170   \n",
      "3                   0           0                     125   \n",
      "4                   0           1                     156   \n",
      "\n",
      "  exercise_induced_angina st_depression peak_exercise_st_segment  \\\n",
      "0                       0             0                        ?   \n",
      "1                       1           2.5                        2   \n",
      "2                       0             0                        1   \n",
      "3                       0             0                        ?   \n",
      "4                       1             0                        1   \n",
      "\n",
      "  num_major_flourosopy thal  heart_disease_diagnosis  \n",
      "0                    ?    ?                        1  \n",
      "1                    ?    ?                        1  \n",
      "2                    0    3                        0  \n",
      "3                    ?    ?                        0  \n",
      "4                    ?    6                        3  \n",
      "\n",
      "Find sum value undefined in each column\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "age                         0\n",
       "sex                         0\n",
       "chest_pain_type             0\n",
       "resting_blood_pressure      0\n",
       "serum_cholesterol           0\n",
       "fasting_blood_sugar         0\n",
       "resting_ecg                 1\n",
       "max_heart_rate_achieved     0\n",
       "exercise_induced_angina     0\n",
       "st_depression               0\n",
       "peak_exercise_st_segment    0\n",
       "num_major_flourosopy        0\n",
       "thal                        0\n",
       "heart_disease_diagnosis     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Column data heart train')\n",
    "pprint(heart_train.dtypes)\n",
    "print()\n",
    "print('Show heart train head')\n",
    "pprint(heart_train.head())\n",
    "print()\n",
    "print('Find sum value undefined in each column')\n",
    "heart_train.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From above results, we can conclude that data given from CSV:\n",
    "1. Not all data is numeric\n",
    "2. There are some datas which value is '?'\n",
    "3. There are also some datas which value is undefined (NaN)\n",
    "\n",
    "**Our conclusion**: \n",
    "\n",
    "Results above can disturb the modeling process. Because of that, we need to do some pre-processing to ensure that there are little to no noise in the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataframe Conversion to Numeric "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data type of columns after conversion\n",
      "age                           int64\n",
      "sex                           int64\n",
      "chest_pain_type               int64\n",
      "resting_blood_pressure      float64\n",
      "serum_cholesterol           float64\n",
      "fasting_blood_sugar         float64\n",
      "resting_ecg                 float64\n",
      "max_heart_rate_achieved     float64\n",
      "exercise_induced_angina     float64\n",
      "st_depression               float64\n",
      "peak_exercise_st_segment    float64\n",
      "num_major_flourosopy        float64\n",
      "thal                        float64\n",
      "heart_disease_diagnosis       int64\n",
      "dtype: object\n",
      "\n",
      "Total value NaN after heart_train converted to numeric value\n",
      "age                           0\n",
      "sex                           0\n",
      "chest_pain_type               0\n",
      "resting_blood_pressure       47\n",
      "serum_cholesterol            24\n",
      "fasting_blood_sugar          78\n",
      "resting_ecg                   2\n",
      "max_heart_rate_achieved      44\n",
      "exercise_induced_angina      44\n",
      "st_depression                49\n",
      "peak_exercise_st_segment    262\n",
      "num_major_flourosopy        514\n",
      "thal                        408\n",
      "heart_disease_diagnosis       0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Convert string to numeric\n",
    "heart_train = heart_train.apply(pd.to_numeric, errors = 'coerce')\n",
    "# Print data\n",
    "print('Data type of columns after conversion')\n",
    "print(heart_train.dtypes)\n",
    "print()\n",
    "# Show NaN count\n",
    "print('Total value NaN after heart_train converted to numeric value')\n",
    "print(heart_train.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing NaN Values\n",
    "\n",
    "In the process above, first, we convert the data from object (string) to numeric value. We succeed in removing object value from dataframe. But there are also a problem, value which can not be convered to numeric data type will be converted to NaN and that problem can make dataframe hard to be processed.\n",
    "\n",
    "One of the easiest way to remove NaN value is to remove row which contains NaN value in it. But that way is not really good and not feasible, since column 12 has 514 rows which value is NaN and that means removing 514 rows which will reduce many data trainings. Another way is to replace NaN value with median. We choose median value rather than mean because median value is much more stable thant mean for irregular data (outliers).\n",
    "\n",
    "For categorical data, for example \n",
    "Akan tetapi, pengisian data yang bersifat categorical tidak bisa dilakukan dengan nilai median karena akan menghasilkan nilai yang tidak bermakna (bukan termasuk kategori yang ada). Oleh karena itu, khusus untuk atribut-atribut yang bersifat kategori, digunakan modus untuk mengganti nilai-nilai yang ilang.\n",
    "\n",
    "Terakhir, untuk mengurangi noise pada data, dilakukan penghapusan baris-baris yang memiliki nilai NaN pada lebih dari 3 atribut."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop row with too many NaNs\n",
    "heart_train = heart_train.dropna(thresh=10)\n",
    "\n",
    "# # Fill NaN, median for continuous data, mode for categorical data\n",
    "heart_train['thal'].fillna(heart_train['thal'].mode()[0], inplace=True)\n",
    "heart_train['peak_exercise_st_segment'].fillna(heart_train['peak_exercise_st_segment'].mode()[0], inplace=True)\n",
    "heart_train['chest_pain_type'].fillna(heart_train['chest_pain_type'].mode()[0], inplace=True)\n",
    "heart_train['resting_ecg'].fillna(heart_train['resting_ecg'].mode()[0], inplace=True)\n",
    "heart_train['fasting_blood_sugar'].fillna(heart_train['fasting_blood_sugar'].mode()[0], inplace=True)\n",
    "\n",
    "imp = SimpleImputer(missing_values=np.nan, strategy='median')\n",
    "\n",
    "c = heart_train.columns\n",
    "heart_train = pd.DataFrame(imp.fit_transform(heart_train))\n",
    "heart_train.columns = c\n",
    "\n",
    "# Count NaN value\n",
    "print('Total NaN Value')\n",
    "print(heart_train.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting of Categorical Data\n",
    "\n",
    "Pada beberapa data yang bersifat kategori, kategori-kategori yang mungkin direpresentasikan dengan nilai 1-X. akan tetapi, nilai-nilai tersebut mengimplikasikan adanya ordering antara kategori-kategori tersebut (1 < 2, berarti kategori 1 lebih baik/buruk dari kategori 2). Oleh karena itu, pada atribut-atribut tertentu, dilakukan pemecahan atribut menjadi X atribut binary baru, dengan X berupa jumlah kategori yang mungkin untuk atribut tersebut."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upsloping(row):\n",
    "    if row[\"peak_exercise_st_segment\"] == 1: return 1\n",
    "    else: return 0\n",
    "\n",
    "def flat(row):\n",
    "    if row[\"peak_exercise_st_segment\"] == 2: return 1\n",
    "    else: return 0\n",
    "    \n",
    "def downsloping(row):\n",
    "    if row[\"peak_exercise_st_segment\"] == 3: return 1\n",
    "    else: return 0\n",
    "    \n",
    "heart_train['upsloping'] = heart_train.apply(lambda row: row[\"peak_exercise_st_segment\"] == 1, axis=1).astype(int)\n",
    "heart_train['flat'] = heart_train.apply(lambda row: row[\"peak_exercise_st_segment\"] == 2, axis=1).astype(int)\n",
    "heart_train['downsloping'] = heart_train.apply(lambda row: row[\"peak_exercise_st_segment\"] == 3, axis=1).astype(int)\n",
    "heart_train = heart_train.drop('peak_exercise_st_segment', axis=1)\n",
    "heart_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binarizing Data\n",
    "\n",
    "Untuk data-data seperti resting blood pressure dan resting ecg, dilakukan binarisasi data (mengubah data menjadi nilai 1 atau 0). Hal ini dilakukan karena kedua kategori tersebut bisa disimplifikasi menjadi kategori sehat atau tidak (1 atau 0). Hal ini membuat data lebih simpel ketimbang sebelum dilakukan binarisasi, misalkan, untuk resting blood pressure, tanpa binarisasi, model-model yang dilatih harus bisa menemukan weight yang bisa memisahkan antara nilai pada range [120, 140] dan sisanya."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binarize resting_ecg, normal as 0 and everything else as 1\n",
    "heart_train['resting_ecg'] = (heart_train['resting_ecg'] >= 1).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binarize resting blood pressure to 0 for [120, 140] (healthy) and 1 otherwise\n",
    "heart_train['resting_blood_pressure'] = ((heart_train['resting_blood_pressure'] > 140) | (heart_train['resting_blood_pressure'] < 120)).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data train\n",
    "# heart_train_copy = heart_train.copy()\n",
    "Y = heart_train['heart_disease_diagnosis']\n",
    "X = heart_train.drop('heart_disease_diagnosis', axis = 1)\n",
    "\n",
    "# scaler = StandardScaler().fit(X)\n",
    "# X = scaler.transform(X)\n",
    "\n",
    "# Best so far\n",
    "scaler = MinMaxScaler(feature_range=(0,1))\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "KF = KFold(10, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian Naive-Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnb = GaussianNB()\n",
    "\n",
    "results = cross_validate(gnb, X, Y, cv=10)\n",
    "pprint(results)\n",
    "\n",
    "print(sum(results['test_score'])/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=1\n",
    "sum_acc = 0\n",
    "sum_prec = 0\n",
    "sum_rec = 0\n",
    "for trainidx, testidx in KF.split(X):\n",
    "    gnb = GaussianNB()\n",
    "    \n",
    "    X_train, X_test = X[trainidx], X[testidx]\n",
    "    Y_train, Y_test = Y[trainidx], Y[testidx]\n",
    "    gnb.fit(X_train,Y_train)\n",
    "\n",
    "    accuration = metrics.accuracy_score(Y_test, gnb.predict(X_test))\n",
    "    precision = metrics.precision_score(Y_test, gnb.predict(X_test), average=\"macro\")\n",
    "    recall = metrics.recall_score(Y_test, gnb.predict(X_test), average=\"macro\")\n",
    "    \n",
    "    i+=1\n",
    "    sum_acc += accuration\n",
    "    sum_prec += precision\n",
    "    sum_rec += recall\n",
    "\n",
    "results = cross_validate(gnb, X, Y, cv=10)\n",
    "pprint(results)\n",
    "\n",
    "print(sum(results['test_score'])/10)\n",
    "\n",
    "print(\"Average Accuration : {0:.4f}\".format(sum_acc/10))\n",
    "print(\"Average Precision : {0:.4f}\".format(sum_prec/10))\n",
    "print(\"Average Recall : {0:.4f}\".format(sum_rec/10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = tree.DecisionTreeClassifier()\n",
    "\n",
    "results = cross_validate(dt, X, Y, cv=10)\n",
    "pprint(results)\n",
    "\n",
    "print(sum(results['test_score'])/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=1\n",
    "sum_acc = 0\n",
    "sum_prec = 0\n",
    "sum_rec = 0\n",
    "for trainidx, testidx in KF.split(X):\n",
    "    dt = tree.DecisionTreeClassifier()\n",
    "    \n",
    "    X_train, X_test = X[trainidx], X[testidx]\n",
    "    Y_train, Y_test = Y[trainidx], Y[testidx]\n",
    "    dt.fit(X_train,Y_train)\n",
    "\n",
    "    accuration = metrics.accuracy_score(Y_test, dt.predict(X_test))\n",
    "    precision = metrics.precision_score(Y_test, dt.predict(X_test), average=\"macro\")\n",
    "    recall = metrics.recall_score(Y_test, dt.predict(X_test), average=\"macro\")\n",
    "    \n",
    "    i+=1\n",
    "    sum_acc += accuration\n",
    "    sum_prec += precision\n",
    "    sum_rec += recall\n",
    "    \n",
    "results = cross_validate(dt, X, Y, cv=10)\n",
    "pprint(results)\n",
    "\n",
    "print(sum(results['test_score'])/10)\n",
    "\n",
    "print(\"Average Accuration : {0:.4f}\".format(sum_acc/10))\n",
    "print(\"Average Precision : {0:.4f}\".format(sum_prec/10))\n",
    "print(\"Average Recall : {0:.4f}\".format(sum_rec/10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=44, weights='distance')\n",
    "\n",
    "results = cross_validate(knn, X, Y, cv=5)\n",
    "pprint(results)\n",
    "\n",
    "print(sum(results['test_score'])/5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=1\n",
    "sum_acc = 0\n",
    "sum_prec = 0\n",
    "sum_rec = 0\n",
    "for trainidx, testidx in KF.split(X):\n",
    "    knn = KNeighborsClassifier(n_neighbors=44, weights='distance')\n",
    "    \n",
    "    X_train, X_test = X[trainidx], X[testidx]\n",
    "    Y_train, Y_test = Y[trainidx], Y[testidx]\n",
    "    knn.fit(X_train,Y_train)\n",
    "\n",
    "    accuration = metrics.accuracy_score(Y_test, knn.predict(X_test))\n",
    "    precision = metrics.precision_score(Y_test, knn.predict(X_test), average=\"macro\")\n",
    "    recall = metrics.recall_score(Y_test, knn.predict(X_test), average=\"macro\")\n",
    "    \n",
    "    i+=1\n",
    "    sum_acc += accuration\n",
    "    sum_prec += precision\n",
    "    sum_rec += recall\n",
    "    \n",
    "results = cross_validate(knn, X, Y, cv=5)\n",
    "pprint(results)\n",
    "\n",
    "print(sum(results['test_score'])/5)\n",
    "    \n",
    "print(\"Average Accuration : {0:.4f}\".format(sum_acc/10))\n",
    "print(\"Average Precision : {0:.4f}\".format(sum_prec/10))\n",
    "print(\"Average Recall : {0:.4f}\".format(sum_rec/10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "mlp = MLPClassifier(hidden_layer_sizes=(2), solver='sgd', \n",
    "                    max_iter=1000, learning_rate_init=0.1, learning_rate='adaptive',\n",
    "                    activation='identity')\n",
    "\n",
    "results = cross_validate(mlp, X, Y, cv=10)\n",
    "pprint(results)\n",
    "\n",
    "print(sum(results['test_score'])/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=1\n",
    "sum_acc = 0\n",
    "sum_prec = 0\n",
    "sum_rec = 0\n",
    "for trainidx, testidx in KF.split(X):\n",
    "    mlp = MLPClassifier(hidden_layer_sizes=(2), solver='sgd', \n",
    "                        max_iter=1000, learning_rate_init=0.1, learning_rate='adaptive',\n",
    "                        activation='identity')\n",
    "    \n",
    "    X_train, X_test = X[trainidx], X[testidx]\n",
    "    Y_train, Y_test = Y[trainidx], Y[testidx]\n",
    "    mlp.fit(X_train,Y_train)\n",
    "\n",
    "    accuration = metrics.accuracy_score(Y_test, mlp.predict(X_test))\n",
    "    precision = metrics.precision_score(Y_test, mlp.predict(X_test), average=\"macro\")\n",
    "    recall = metrics.recall_score(Y_test, mlp.predict(X_test), average=\"macro\")\n",
    "    \n",
    "    i+=1\n",
    "    sum_acc += accuration\n",
    "    sum_prec += precision\n",
    "    sum_rec += recall\n",
    "    \n",
    "results = cross_validate(mlp, X, Y, cv=10)\n",
    "pprint(results)\n",
    "\n",
    "print(sum(results['test_score'])/10)\n",
    "    \n",
    "print(\"Average Accuration : {0:.4f}\".format(sum_acc/10))\n",
    "print(\"Average Precision : {0:.4f}\".format(sum_prec/10))\n",
    "print(\"Average Recall : {0:.4f}\".format(sum_rec/10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model using Joblib\n",
    "# Pilih salah satu yg terbaik\n",
    "\n",
    "#joblib.dump(gnb, 'naivebayes.pkl')\n",
    "#joblib.dump(tree, 'decisiontree.pkl')\n",
    "joblib.dump(knn, 'knn.pkl')\n",
    "#joblib.dump(mlp, 'mlp.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model using Joblib\n",
    "# Pilih salah satu yg terbaik\n",
    "\n",
    "#gnb = joblib.load('naivebayes.pkl')\n",
    "#tree = joblib.load('decisiontree.pkl')\n",
    "knn = joblib.load('knn.pkl')\n",
    "#mlp = joblib.load('mlp.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluasi Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
