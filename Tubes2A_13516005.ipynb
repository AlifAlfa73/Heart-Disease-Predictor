{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignments IF3170 - Artificial Intelligence\n",
    "## Web Application for classifying hearth disease from data clinic \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase A - Find Best Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Group 3 - Unexpected\n",
    "    - Rizki Alif Salman Alfarisy / 13516005\n",
    "    - Jonathan Alvaro / 13516023\n",
    "    - Joseph Salimin / 13516037\n",
    "    - Kevin Leonardo Limitius / 13516049\n",
    "    - Kevin Basuki / 13516071"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First of All.. Import All Library Needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import pickle\n",
    "import itertools\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, train_test_split, KFold\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn import metrics\n",
    "from sklearn import tree\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error, classification_report, accuracy_score, confusion_matrix\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "from pprint import pprint\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Data From CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "heart_train = pd.read_csv('tubes2_HeartDisease_train.csv')\n",
    "heart_test = pd.read_csv('tubes2_HeartDisease_test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rename Column Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "test_columns = {\n",
    "    'Column1': 'age',\n",
    "    'Column2': 'sex',\n",
    "    'Column3': 'chest_pain_type',\n",
    "    'Column4': 'resting_blood_pressure',\n",
    "    'Column5': 'serum_cholesterol',\n",
    "    'Column6': 'fasting_blood_sugar',\n",
    "    'Column7': 'resting_ecg',\n",
    "    'Column8': 'max_heart_rate_achieved',\n",
    "    'Column9': 'exercise_induced_angina',\n",
    "    'Column10': 'st_depression',\n",
    "    'Column11': 'peak_exercise_st_segment',\n",
    "    'Column12': 'num_major_flourosopy',\n",
    "    'Column13': 'thal'\n",
    "}\n",
    "# Create train columns\n",
    "train_columns = test_columns.copy()\n",
    "train_columns['Column14'] = 'heart_disease_diagnosis'\n",
    "\n",
    "# Rename columns\n",
    "heart_train = heart_train.rename(columns=train_columns)\n",
    "heart_test = heart_test.rename(columns=test_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column data heart train\n",
      "age                          int64\n",
      "sex                          int64\n",
      "chest_pain_type              int64\n",
      "resting_blood_pressure      object\n",
      "serum_cholesterol           object\n",
      "fasting_blood_sugar         object\n",
      "resting_ecg                 object\n",
      "max_heart_rate_achieved     object\n",
      "exercise_induced_angina     object\n",
      "st_depression               object\n",
      "peak_exercise_st_segment    object\n",
      "num_major_flourosopy        object\n",
      "thal                        object\n",
      "heart_disease_diagnosis      int64\n",
      "dtype: object\n",
      "\n",
      "Show heart train head\n",
      "   age  sex  chest_pain_type resting_blood_pressure serum_cholesterol  \\\n",
      "0   54    1                4                    125               216   \n",
      "1   55    1                4                    158               217   \n",
      "2   54    0                3                    135               304   \n",
      "3   48    0                3                    120               195   \n",
      "4   50    1                4                    120                 0   \n",
      "\n",
      "  fasting_blood_sugar resting_ecg max_heart_rate_achieved  \\\n",
      "0                   0           0                     140   \n",
      "1                   0           0                     110   \n",
      "2                   1           0                     170   \n",
      "3                   0           0                     125   \n",
      "4                   0           1                     156   \n",
      "\n",
      "  exercise_induced_angina st_depression peak_exercise_st_segment  \\\n",
      "0                       0             0                        ?   \n",
      "1                       1           2.5                        2   \n",
      "2                       0             0                        1   \n",
      "3                       0             0                        ?   \n",
      "4                       1             0                        1   \n",
      "\n",
      "  num_major_flourosopy thal  heart_disease_diagnosis  \n",
      "0                    ?    ?                        1  \n",
      "1                    ?    ?                        1  \n",
      "2                    0    3                        0  \n",
      "3                    ?    ?                        0  \n",
      "4                    ?    6                        3  \n",
      "\n",
      "Find sum value undefined in each column\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "age                         0\n",
       "sex                         0\n",
       "chest_pain_type             0\n",
       "resting_blood_pressure      0\n",
       "serum_cholesterol           0\n",
       "fasting_blood_sugar         0\n",
       "resting_ecg                 1\n",
       "max_heart_rate_achieved     0\n",
       "exercise_induced_angina     0\n",
       "st_depression               0\n",
       "peak_exercise_st_segment    0\n",
       "num_major_flourosopy        0\n",
       "thal                        0\n",
       "heart_disease_diagnosis     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Column data heart train')\n",
    "pprint(heart_train.dtypes)\n",
    "print()\n",
    "print('Show heart train head')\n",
    "pprint(heart_train.head())\n",
    "print()\n",
    "print('Find sum value undefined in each column')\n",
    "heart_train.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From above results, we can conclude that data given from CSV:\n",
    "1. Not all data is numeric\n",
    "2. There are some datas which value is '?'\n",
    "3. There are also some datas which value is undefined (NaN)\n",
    "\n",
    "**Our conclusion**: \n",
    "\n",
    "Results above can disturb the modeling process. Because of that, we need to do some pre-processing to ensure that there are little to no noise in the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataframe Conversion to Numeric "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data type of columns after conversion\n",
      "age                           int64\n",
      "sex                           int64\n",
      "chest_pain_type               int64\n",
      "resting_blood_pressure      float64\n",
      "serum_cholesterol           float64\n",
      "fasting_blood_sugar         float64\n",
      "resting_ecg                 float64\n",
      "max_heart_rate_achieved     float64\n",
      "exercise_induced_angina     float64\n",
      "st_depression               float64\n",
      "peak_exercise_st_segment    float64\n",
      "num_major_flourosopy        float64\n",
      "thal                        float64\n",
      "heart_disease_diagnosis       int64\n",
      "dtype: object\n",
      "\n",
      "Total value NaN after heart_train converted to numeric value\n",
      "age                           0\n",
      "sex                           0\n",
      "chest_pain_type               0\n",
      "resting_blood_pressure       47\n",
      "serum_cholesterol            24\n",
      "fasting_blood_sugar          78\n",
      "resting_ecg                   2\n",
      "max_heart_rate_achieved      44\n",
      "exercise_induced_angina      44\n",
      "st_depression                49\n",
      "peak_exercise_st_segment    262\n",
      "num_major_flourosopy        514\n",
      "thal                        408\n",
      "heart_disease_diagnosis       0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Convert string to numeric\n",
    "heart_train = heart_train.apply(pd.to_numeric, errors = 'coerce')\n",
    "# Print data\n",
    "print('Data type of columns after conversion')\n",
    "print(heart_train.dtypes)\n",
    "print()\n",
    "# Show NaN count\n",
    "print('Total value NaN after heart_train converted to numeric value')\n",
    "print(heart_train.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing NaN Values\n",
    "\n",
    "In the process above, first, we convert the data from object (string) to numeric value. We succeed in removing object value from dataframe. But there are also a problem, value which can not be convered to numeric data type will be converted to NaN and that problem can make dataframe hard to be processed.\n",
    "\n",
    "First of all, to reduce noise in data, we try to remove row which has more than 3 NaNs in its attribute columns.\n",
    "\n",
    "One of the easiest way to remove NaN value is to remove row which contains NaN value in it. But that way is not really good and not feasible, since Column 12 has 514 rows which value is NaN and that means removing 514 rows which will reduce many data trainings. Another way is to replace NaN value with median. We choose median value rather than mean because median value is much more stable thant mean for irregular data (outliers).\n",
    "\n",
    "For categorical data, for example in Column 3 and Column 7, it is better to replace NaN value with mode, since replacing the valie with median will result in creating meaningless value (not in that category). That's why, for Column 3, 6, 7, 11, and 13, we replace the value with mode.\n",
    "\n",
    "During the imputation process, in order to ensure that the missing data is not filled in by overgeneralized values, we first separate the training data into 5 different clusters, one for each possible output class. Then, we impute the missing values separately for each cluster before finally the clusters are once again merged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total NaN Value\n",
      "age                           0\n",
      "sex                           0\n",
      "chest_pain_type               0\n",
      "resting_blood_pressure        2\n",
      "serum_cholesterol            21\n",
      "fasting_blood_sugar          77\n",
      "resting_ecg                   2\n",
      "max_heart_rate_achieved       0\n",
      "exercise_induced_angina       0\n",
      "st_depression                 5\n",
      "peak_exercise_st_segment    218\n",
      "num_major_flourosopy        469\n",
      "thal                        366\n",
      "heart_disease_diagnosis       0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Drop row with too many NaNs\n",
    "heart_train = heart_train.dropna(thresh=10)\n",
    "\n",
    "# Count NaN value\n",
    "print('Total NaN Value')\n",
    "print(heart_train.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cluster dataset\n",
    "heart_train_0 = heart_train[heart_train[\"heart_disease_diagnosis\"] == 0].copy()\n",
    "heart_train_1 = heart_train[heart_train[\"heart_disease_diagnosis\"] == 1].copy()\n",
    "heart_train_2 = heart_train[heart_train[\"heart_disease_diagnosis\"] == 2].copy()\n",
    "heart_train_3 = heart_train[heart_train[\"heart_disease_diagnosis\"] == 3].copy()\n",
    "heart_train_4 = heart_train[heart_train[\"heart_disease_diagnosis\"] == 4].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_nan_with_value(train):\n",
    "    train['peak_exercise_st_segment'].fillna(train['peak_exercise_st_segment'].mode()[0], inplace=True)\n",
    "    train['chest_pain_type'].fillna(train['chest_pain_type'].mode()[0], inplace=True)\n",
    "    train['resting_ecg'].fillna(train['resting_ecg'].mode()[0], inplace=True)\n",
    "    train['fasting_blood_sugar'].fillna(train['fasting_blood_sugar'].mode()[0], inplace=True)  \n",
    "    train['thal'].fillna(train['thal'].mode()[0], inplace=True)\n",
    "    imp = SimpleImputer(missing_values=np.nan, strategy='median')\n",
    "    c = train.columns\n",
    "    train = pd.DataFrame(imp.fit_transform(train))\n",
    "    train.columns = c\n",
    "    return train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total NaN Value\n",
      "age                         0\n",
      "sex                         0\n",
      "chest_pain_type             0\n",
      "resting_blood_pressure      0\n",
      "serum_cholesterol           0\n",
      "fasting_blood_sugar         0\n",
      "resting_ecg                 0\n",
      "max_heart_rate_achieved     0\n",
      "exercise_induced_angina     0\n",
      "st_depression               0\n",
      "peak_exercise_st_segment    0\n",
      "num_major_flourosopy        0\n",
      "thal                        0\n",
      "heart_disease_diagnosis     0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "heart_train_0 = fill_nan_with_value(heart_train_0)\n",
    "heart_train_1 = fill_nan_with_value(heart_train_1)\n",
    "heart_train_2 = fill_nan_with_value(heart_train_2)\n",
    "heart_train_3 = fill_nan_with_value(heart_train_3)\n",
    "heart_train_4 = fill_nan_with_value(heart_train_4)\n",
    "\n",
    "heart_train = pd.concat([heart_train_0, heart_train_1, heart_train_2, heart_train_3, heart_train_4])\n",
    "\n",
    "heart_train = heart_train.reset_index()\n",
    "del heart_train['index']\n",
    "\n",
    "# Count NaN value\n",
    "print('Total NaN Value')\n",
    "print(heart_train.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Correlation and Remove Unnecessery Columns\n",
    "\n",
    "Then, we want to check the correlation of the column and the result. The results is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3573179931724326"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "heart_train['age'].corr(heart_train['heart_disease_diagnosis'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2557223264055122"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "heart_train['sex'].corr(heart_train['heart_disease_diagnosis'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3948898817281978"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "heart_train['chest_pain_type'].corr(heart_train['heart_disease_diagnosis'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1171081225091242"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "heart_train['resting_blood_pressure'].corr(heart_train['heart_disease_diagnosis'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.2532051886557461"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "heart_train['serum_cholesterol'].corr(heart_train['heart_disease_diagnosis'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1035208549842671"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "heart_train['fasting_blood_sugar'].corr(heart_train['heart_disease_diagnosis'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.14898347335673523"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "heart_train['resting_ecg'].corr(heart_train['heart_disease_diagnosis'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.36620039325574066"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "heart_train['max_heart_rate_achieved'].corr(heart_train['heart_disease_diagnosis'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3924863582518764"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "heart_train['exercise_induced_angina'].corr(heart_train['heart_disease_diagnosis'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.23000199179131073"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "heart_train['st_depression'].corr(heart_train['heart_disease_diagnosis'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.46570768383507927"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "heart_train['peak_exercise_st_segment'].corr(heart_train['heart_disease_diagnosis'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5765972104207245"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "heart_train['thal'].corr(heart_train['heart_disease_diagnosis'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete Unneccesary Columns\n",
    "\n",
    "From results above, then we want to delete columns which are not really necessary. The column which has correlation between -0.25 < x < 0.25 will be removed by us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "del heart_train['resting_blood_pressure']\n",
    "del heart_train['fasting_blood_sugar']\n",
    "del heart_train['resting_ecg']\n",
    "del heart_train['st_depression']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data train\n",
    "# heart_train_copy = heart_train.copy()\n",
    "Y = heart_train['heart_disease_diagnosis']\n",
    "X = heart_train.drop('heart_disease_diagnosis', axis = 1).values\n",
    "\n",
    "ros = RandomOverSampler(random_state=0)\n",
    "X, Y = ros.fit_resample(X, Y)\n",
    "\n",
    "# Best so far\n",
    "scaler = MinMaxScaler(feature_range=(0,1))\n",
    "X = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "KF = StratifiedKFold(10, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find Best Model\n",
    "\n",
    "From dataset we have pre processed above, then we create model and find which model can give best performance (not just accuracy, but also the behaviour of the model too)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian Naive-Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Accuracy : 0.5667\n",
      "Average Precision : 0.5590\n",
      "Average Recall : 0.5667\n"
     ]
    }
   ],
   "source": [
    "sum_acc, sum_prec, sum_rec = 0, 0, 0\n",
    "list_gnb_model = []\n",
    "\n",
    "for trainidx, testidx in KF.split(X, Y):\n",
    "    # Create GNB Model and test it\n",
    "    gnb = GaussianNB()\n",
    "    X_train, X_test = X[trainidx], X[testidx]\n",
    "    Y_train, Y_test = Y[trainidx], Y[testidx]\n",
    "    gnb.fit(X_train,Y_train)\n",
    "    # Calculate accuraction, precision, and recall value\n",
    "    accuracy = metrics.accuracy_score(Y_test, gnb.predict(X_test))\n",
    "    precision = metrics.precision_score(Y_test, gnb.predict(X_test), average=\"macro\")\n",
    "    recall = metrics.recall_score(Y_test, gnb.predict(X_test), average=\"macro\")\n",
    "    # Append it to list\n",
    "    list_gnb_model.append((gnb, accuracy, precision, recall))\n",
    "    sum_acc += accuracy\n",
    "    sum_prec += precision\n",
    "    sum_rec += recall\n",
    "    \n",
    "# Find best model based on accuracy\n",
    "best_model_index = 0\n",
    "for i in range(1, len(list_gnb_model)):\n",
    "    if list_gnb_model[i][1] > list_gnb_model[best_model_index][1]:\n",
    "        best_model_index = i\n",
    "gnb = list_gnb_model[best_model_index][0]\n",
    "    \n",
    "print(\"Average Accuracy : {0:.4f}\".format(sum_acc/10))\n",
    "print(\"Average Precision : {0:.4f}\".format(sum_prec/10))\n",
    "print(\"Average Recall : {0:.4f}\".format(sum_rec/10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Accuracy : 0.9396\n",
      "Average Precision : 0.9421\n",
      "Average Recall : 0.9396\n"
     ]
    }
   ],
   "source": [
    "sum_acc, sum_prec, sum_rec = 0, 0, 0\n",
    "list_dt_model = []\n",
    "\n",
    "for trainidx, testidx in KF.split(X, Y):\n",
    "    # Create Tree Model and test it\n",
    "    dt = tree.DecisionTreeClassifier()\n",
    "    X_train, X_test = X[trainidx], X[testidx]\n",
    "    Y_train, Y_test = Y[trainidx], Y[testidx]\n",
    "    dt.fit(X_train,Y_train)\n",
    "    # Calculate accuraction, precision, and recall value\n",
    "    accuracy = metrics.accuracy_score(Y_test, dt.predict(X_test))\n",
    "    precision = metrics.precision_score(Y_test, dt.predict(X_test), average=\"macro\")\n",
    "    recall = metrics.recall_score(Y_test, dt.predict(X_test), average=\"macro\")\n",
    "    # Append it to list\n",
    "    list_dt_model.append((dt, accuracy, precision, recall))\n",
    "    sum_acc += accuracy\n",
    "    sum_prec += precision\n",
    "    sum_rec += recall\n",
    "    \n",
    "# Find best model based on accuracy\n",
    "best_model_index = 0\n",
    "for i in range(1, len(list_dt_model)):\n",
    "    if list_dt_model[i][1] > list_dt_model[best_model_index][1]:\n",
    "        best_model_index = i\n",
    "dt = list_dt_model[best_model_index][0]\n",
    "    \n",
    "print(\"Average Accuracy : {0:.4f}\".format(sum_acc/10))\n",
    "print(\"Average Precision : {0:.4f}\".format(sum_prec/10))\n",
    "print(\"Average Recall : {0:.4f}\".format(sum_rec/10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Accuracy : 0.9330\n",
      "Average Precision : 0.9376\n",
      "Average Recall : 0.9330\n"
     ]
    }
   ],
   "source": [
    "sum_acc, sum_prec, sum_rec = 0, 0, 0\n",
    "list_knn_model = []\n",
    "\n",
    "for trainidx, testidx in KF.split(X, Y):\n",
    "    # Create KNN Model and test it\n",
    "    knn = KNeighborsClassifier(n_neighbors=44, weights='distance')\n",
    "    X_train, X_test = X[trainidx], X[testidx]\n",
    "    Y_train, Y_test = Y[trainidx], Y[testidx]\n",
    "    knn.fit(X_train,Y_train)\n",
    "    # Calculate accuraction, precision, and recall value\n",
    "    accuracy = metrics.accuracy_score(Y_test, knn.predict(X_test))\n",
    "    precision = metrics.precision_score(Y_test, knn.predict(X_test), average=\"macro\")\n",
    "    recall = metrics.recall_score(Y_test, knn.predict(X_test), average=\"macro\")\n",
    "    # Append it to list\n",
    "    list_knn_model.append((knn, accuracy, precision, recall))\n",
    "    sum_acc += accuracy\n",
    "    sum_prec += precision\n",
    "    sum_rec += recall    \n",
    "\n",
    "# Find best model based on accuracy\n",
    "best_model_index = 0\n",
    "for i in range(1, len(list_knn_model)):\n",
    "    if list_knn_model[i][1] > list_knn_model[best_model_index][1]:\n",
    "        best_model_index = i\n",
    "knn = list_knn_model[best_model_index][0]\n",
    "    \n",
    "print(\"Average Accuracy : {0:.4f}\".format(sum_acc/10))\n",
    "print(\"Average Precision : {0:.4f}\".format(sum_prec/10))\n",
    "print(\"Average Recall : {0:.4f}\".format(sum_rec/10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Accuracy : 0.5725\n",
      "Average Precision : 0.5591\n",
      "Average Recall : 0.5725\n"
     ]
    }
   ],
   "source": [
    "sum_acc, sum_prec, sum_rec = 0, 0, 0\n",
    "list_mlp_model = []\n",
    "\n",
    "for trainidx, testidx in KF.split(X, Y):\n",
    "    # Create MLP Model and test it\n",
    "    mlp = MLPClassifier(hidden_layer_sizes=(7), solver='sgd', \n",
    "                        max_iter=1000, learning_rate_init=0.1, learning_rate='adaptive',\n",
    "                        activation='identity')\n",
    "    X_train, X_test = X[trainidx], X[testidx]\n",
    "    Y_train, Y_test = Y[trainidx], Y[testidx]\n",
    "    mlp.fit(X_train,Y_train)\n",
    "    # Calculate accuraction, precision, and recall value\n",
    "    accuracy = metrics.accuracy_score(Y_test, mlp.predict(X_test))\n",
    "    precision = metrics.precision_score(Y_test, mlp.predict(X_test), average=\"macro\")\n",
    "    recall = metrics.recall_score(Y_test, mlp.predict(X_test), average=\"macro\")\n",
    "    # Append it to list\n",
    "    list_mlp_model.append((mlp, accuracy, precision, recall))\n",
    "    sum_acc += accuracy\n",
    "    sum_prec += precision\n",
    "    sum_rec += recall\n",
    "\n",
    "# Find best model based on accuracy\n",
    "best_model_index = 0\n",
    "for i in range(1, len(list_mlp_model)):\n",
    "    if list_mlp_model[i][1] > list_mlp_model[best_model_index][1]:\n",
    "        best_model_index = i\n",
    "mlp = list_mlp_model[best_model_index][0]\n",
    "    \n",
    "print(\"Average Accuracy : {0:.4f}\".format(sum_acc/10))\n",
    "print(\"Average Precision : {0:.4f}\".format(sum_prec/10))\n",
    "print(\"Average Recall : {0:.4f}\".format(sum_rec/10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Out of the 4 models that have been trained above, we decided to use the MLP model for several reasons. First, the simplest reason is that it has the highest accuracy. Then, there is also the fact that some attributes of the data have continuous values which is more suitable to be processed by the MLP model instead of the other three."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mlp.pkl']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(mlp, 'mlp.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='identity', alpha=0.0001, batch_size='auto',\n",
       "       beta_1=0.9, beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=7, learning_rate='adaptive',\n",
       "       learning_rate_init=0.1, max_iter=1000, momentum=0.9,\n",
       "       n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
       "       random_state=None, shuffle=True, solver='sgd', tol=0.0001,\n",
       "       validation_fraction=0.1, verbose=False, warm_start=False)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp = joblib.load('mlp.pkl')\n",
    "mlp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre process Data Test\n",
    "\n",
    "After saving model, then we also pre process data test with same steps as data train, the results are shown below here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data type of columns after conversion\n",
      "age                           int64\n",
      "sex                           int64\n",
      "chest_pain_type               int64\n",
      "resting_blood_pressure      float64\n",
      "serum_cholesterol           float64\n",
      "fasting_blood_sugar         float64\n",
      "resting_ecg                   int64\n",
      "max_heart_rate_achieved     float64\n",
      "exercise_induced_angina     float64\n",
      "st_depression               float64\n",
      "peak_exercise_st_segment    float64\n",
      "num_major_flourosopy        float64\n",
      "thal                        float64\n",
      "dtype: object\n",
      "\n",
      "Total value NaN after heart_train converted to numeric value\n",
      "age                          0\n",
      "sex                          0\n",
      "chest_pain_type              0\n",
      "resting_blood_pressure      12\n",
      "serum_cholesterol            6\n",
      "fasting_blood_sugar         12\n",
      "resting_ecg                  0\n",
      "max_heart_rate_achieved     11\n",
      "exercise_induced_angina     11\n",
      "st_depression               13\n",
      "peak_exercise_st_segment    47\n",
      "num_major_flourosopy        97\n",
      "thal                        78\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Convert data string in heart_test to numeric\n",
    "heart_test = heart_test.apply(pd.to_numeric, errors = 'coerce')\n",
    "# Print data\n",
    "print('Data type of columns after conversion')\n",
    "print(heart_test.dtypes)\n",
    "print()\n",
    "# Show NaN count\n",
    "print('Total value NaN after heart_train converted to numeric value')\n",
    "print(heart_test.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill NaN Value for test is same for data train\n",
    "# Mode\n",
    "heart_test['peak_exercise_st_segment'].fillna(heart_test['peak_exercise_st_segment'].mode()[0], inplace=True)\n",
    "heart_test['chest_pain_type'].fillna(heart_test['chest_pain_type'].mode()[0], inplace=True)\n",
    "heart_test['resting_ecg'].fillna(heart_test['resting_ecg'].mode()[0], inplace=True)\n",
    "heart_test['fasting_blood_sugar'].fillna(heart_test['fasting_blood_sugar'].mode()[0], inplace=True)  \n",
    "heart_test['thal'].fillna(heart_test['thal'].mode()[0], inplace=True)\n",
    "# Median\n",
    "imp = SimpleImputer(missing_values=np.nan, strategy='median')\n",
    "c = heart_test.columns\n",
    "heart_test = pd.DataFrame(imp.fit_transform(heart_test))\n",
    "heart_test.columns = c\n",
    "# Remove unnecessary column\n",
    "del heart_test['resting_blood_pressure']\n",
    "del heart_test['fasting_blood_sugar']\n",
    "del heart_test['resting_ecg']\n",
    "del heart_test['st_depression']\n",
    "# Scaler\n",
    "# Best so far\n",
    "scaler_test = MinMaxScaler(feature_range=(0,1))\n",
    "heart_test = scaler_test.fit_transform(heart_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict\n",
    "\n",
    "Then we try to predict data test with our chosen model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 2., 1., 1., 1., 1., 3., 2., 1., 0., 2., 1., 1., 4., 1., 3., 2.,\n",
       "       1., 1., 1., 0., 2., 0., 1., 1., 0., 1., 2., 0., 2., 0., 2., 2., 0.,\n",
       "       1., 0., 0., 1., 0., 0., 1., 1., 0., 1., 2., 1., 1., 2., 1., 1., 1.,\n",
       "       1., 2., 1., 1., 0., 1., 3., 1., 0., 1., 1., 1., 2., 1., 0., 2., 0.,\n",
       "       0., 1., 2., 1., 4., 2., 0., 1., 1., 0., 1., 1., 0., 3., 0., 2., 1.,\n",
       "       2., 0., 0., 1., 1., 3., 0., 1., 0., 1., 1., 1., 1., 1., 0., 4., 0.,\n",
       "       4., 0., 1., 1., 0., 2., 1., 2., 1., 0., 0., 0., 1., 2., 1., 2., 0.,\n",
       "       0., 4., 1., 0., 1., 2., 1., 3., 2., 1., 0., 0., 1., 2., 0., 2., 3.,\n",
       "       0., 1., 0., 1., 0.])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp.predict(heart_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The End\n",
    "### Thank You!!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
